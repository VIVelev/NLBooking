{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe():\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    with sr.Microphone() as src:\n",
    "        audio = recognizer.listen(src)\n",
    "        \n",
    "    try:\n",
    "        res = recognizer.recognize_google(audio).lower()\n",
    "        print(res)\n",
    "        return res\n",
    "    \n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Could not understand audio\")\n",
    "        return None\n",
    "        \n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results from the service; {0}\".format(e))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_string = \"i would like to go to barcelona for a week from tomorrow and i would like my hotel to be near a forest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'barcelona',\n",
       " 'for',\n",
       " 'a',\n",
       " 'week',\n",
       " 'from',\n",
       " 'tomorrow',\n",
       " 'and',\n",
       " 'i',\n",
       " 'would',\n",
       " 'like',\n",
       " 'my',\n",
       " 'hotel',\n",
       " 'to',\n",
       " 'be',\n",
       " 'near',\n",
       " 'a',\n",
       " 'forest']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise removal / data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(a):\n",
    "    noise = stopwords.words(\"english\")   \n",
    "    new = []\n",
    "\n",
    "    for x in word_tokenize(a):\n",
    "        if not(x in noise):\n",
    "            new.append(x)\n",
    "            \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = clean(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['would',\n",
       " 'like',\n",
       " 'go',\n",
       " 'barcelona',\n",
       " 'week',\n",
       " 'tomorrow',\n",
       " 'would',\n",
       " 'like',\n",
       " 'hotel',\n",
       " 'near',\n",
       " 'forest']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('like', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('go', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('barcelona', 'VB'),\n",
       " ('for', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('week', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('tomorrow', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('i', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('like', 'VB'),\n",
       " ('my', 'PRP$'),\n",
       " ('hotel', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('near', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('forest', 'NN')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_preprocessing(a):\n",
    "    tokens = word_tokenize(a)\n",
    "    prepositions = [\"to\", \"visit\", \"in\"]\n",
    "    \n",
    "    for prep in prepositions:\n",
    "        idxs = [i for i, x in enumerate(tokens) if x == prep]\n",
    "        for i in idxs:\n",
    "            if i+1 < len(tokens):\n",
    "                tokens[i+1] = tokens[i+1][0].upper() + tokens[i+1][1:]\n",
    "        \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(ner_preprocessing(search_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barcelona', 'GPE'), ('a week from tomorrow', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "pprint([(x.text, x.label_) for x in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tags(data):\n",
    "    idxs = np.unique([x[0] for x in data.treepositions() if len(x) > 1])\n",
    "    return [list(data[int(i)]) for i in idxs]\n",
    "\n",
    "def get_search_tags(a):\n",
    "    search_tag_parser = nltk.RegexpParser(\"STAG: {<IN><DT>?<NN>}\")\n",
    "    data = search_tag_parser.parse(pos_tag(word_tokenize(a)))\n",
    "    return extract_tags(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('for', 'IN'), ('a', 'DT'), ('week', 'NN')],\n",
       " [('from', 'IN'), ('tomorrow', 'NN')],\n",
       " [('near', 'IN'), ('a', 'DT'), ('forest', 'NN')]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_search_tags(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
